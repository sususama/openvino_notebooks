{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3885b63d",
   "metadata": {},
   "source": [
    "# GSoC pretest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee133ba3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1054aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch.quantization as quantization\n",
    "import matplotlib.pyplot as plt\n",
    "from openvino.runtime import Core\n",
    "from openvino.runtime import serialize\n",
    "from openvino.tools import mo\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from onnxruntime.quantization import QuantType, quantize_dynamic\n",
    "from openvino.tools.pot import IEEngine, load_model, save_model, compress_model_weights, create_pipeline\n",
    "from openvino.tools.pot.algorithms.quantization.default.algorithm import DefaultQuantization\n",
    "from openvino.tools.pot.api import DataLoader\n",
    "from openvino.runtime import Core\n",
    "import os\n",
    "import cv2 as cv\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcc8bcd",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c9d001",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_names = timm.list_models('swin_small*')\n",
    "for model_name in model_names:\n",
    "    print(model_name)\n",
    "model = timm.create_model('swin_small_patch4_window7_224', pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809d45df",
   "metadata": {},
   "source": [
    "## Run model at ont-quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3069f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image and preprocess\n",
    "image = Image.open('../notebooks/data/image/coco.jpg')\n",
    "# plt.imshow(image)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.299, 0.224, 0.255])\n",
    "])\n",
    "image = transform(image).unsqueeze(0)\n",
    "\n",
    "# Predict the class of image\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "    pred = output.argmax(dim=1).item()\n",
    "    print(f'Predicted class: {pred}')\n",
    "# After searching, the 208th category in ImageNet is dog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ca845d",
   "metadata": {},
   "source": [
    "## Define convert model to onnx and IR function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5243d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_models(model, model_input, path):\n",
    "    script_model = torch.jit.trace(model, model_input)\n",
    "    torch.onnx.export(script_model,  model_input, path)   \n",
    "    convert_model = mo.convert_model(path)\n",
    "    # Change the .onnx suffix to .xml\n",
    "    IR_path = path[:-4]+'xml'\n",
    "    serialize(convert_model, IR_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b00144",
   "metadata": {},
   "source": [
    "## Convert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95ed983",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert model to onnx and IR\n",
    "# If you already have a models directory, place comment the next line of code.\n",
    "!mkdir models\n",
    "model.eval()\n",
    "onnx_path = 'models/swin_small_patch4_window7_224.onnx'\n",
    "model_input = torch.randn(1,3,224,224).cpu() \n",
    "convert_models(model, model_input,onnx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec63c60",
   "metadata": {},
   "source": [
    "### quantize pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8576f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_int8 = quantization.quantize_dynamic(model, dtype=torch.qint8)\n",
    "with torch.no_grad():\n",
    "    output = model_int8(image)\n",
    "    pred = output.argmax(dim=1).item()\n",
    "    print(f'Predicted class: {pred}')\n",
    "torch.save(model_int8, 'models/quantize_swin.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3df8ac6",
   "metadata": {},
   "source": [
    "### quantize onnx model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantize_path = 'models/quantize_swin.onnx'\n",
    "quantize_dynamic(model_input=onnx_path,\n",
    "    model_output= quantize_path,\n",
    "    weight_type=QuantType.QInt8,\n",
    "    optimize_model=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae025bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(onnx_path)\n",
    "onnx.checker.check_model(onnx_model, full_check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f376c77c",
   "metadata": {},
   "source": [
    "### quantize openvino model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b921b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinDataLoader(DataLoader):\n",
    "    def __init__(self, dataset_path):\n",
    "        self._files = []\n",
    "        all_files_in_dir = os.listdir(dataset_path)\n",
    "        for name in  all_files_in_dir:\n",
    "            file = os.path.join(dataset_path, name)\n",
    "            if cv.haveImageReader(file):\n",
    "                self._files.append(file)\n",
    "\n",
    "        self._shape = (1,3, 224,224)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image =  Image.open(self._files[index])\n",
    "        image = transform(image).unsqueeze(0)\n",
    "        return image, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6bd9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_config = [{\n",
    "    'name': 'DefaultQuantization',\n",
    "    'params':{\n",
    "        'target_device': 'ANY',\n",
    "        'stat_subset_size':300,\n",
    "        'stat_batch_size':1,\n",
    "    },\n",
    "}]\n",
    "model_config = {\n",
    "    \"model_name\": \"model\",\n",
    "    \"model\": 'models/swin_small_patch4_window7_224.xml',\n",
    "    \"weights\": 'models/swin_small_patch4_window7_224.bin'\n",
    "}\n",
    "ie = Core()\n",
    "engine_config = {'device': 'CPU'}\n",
    "\n",
    "data_loader = SwinDataLoader('images/')\n",
    "\n",
    "openvino_model = load_model(model_config=model_config)\n",
    "\n",
    "engine = IEEngine(config=engine_config, data_loader=data_loader)\n",
    "pipline = create_pipeline(algorithm_config, engine)\n",
    "compressed_model = pipline.run(model=openvino_model)\n",
    "compress_model_weights(compressed_model)\n",
    "compressed_model_paths = save_model(\n",
    "    model=compressed_model,\n",
    "    save_path='models/',\n",
    "    model_name = 'quantization_swin'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b57327",
   "metadata": {},
   "source": [
    "### Compara the Size of the Original and Quantized Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8dbbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model_size = Path('models/swin_small_patch4_window7_224.bin').stat().st_size / (1024 * 1024)\n",
    "quantized_model_size = Path('models/quantization_swin.bin').stat().st_size / (1024 * 1024)\n",
    "\n",
    "print(f'Original IR  size: {original_model_size:.2f} MB')\n",
    "print(f'Quantized IR size: {quantized_model_size:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f175729",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model_size = Path('./models/swin_small_patch4_window7_224.onnx').stat().st_size / (1024 * 1024)\n",
    "quantized_model_size = Path('./models/quantize_swin.onnx').stat().st_size / (1024 * 1024)\n",
    "\n",
    "print(f'Original ONNX Model size: {original_model_size:.2f} MB')\n",
    "print(f'Quantized ONNX Model size: {quantized_model_size:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d443cec5",
   "metadata": {},
   "source": [
    "### Compare Performance of the Original and Quantized Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76d3c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "!benchmark_app --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662f82e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!benchmark_app -m models/swin_small_patch4_window7_224.xml -d CPU -t 15 -api sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b17ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!benchmark_app -m models/quantization_swin.xml -d CPU -t 15 -api sync"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9569ae",
   "metadata": {},
   "source": [
    "## quantize to 4 bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e953b229",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/openvino_env/lib/python3.8/site-packages/openvino/tools/pot/algorithms/quantization/utils.py:318: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  input_shape = mode(activations_statistics[input_node_name]['shape'])[0][0]\n"
     ]
    }
   ],
   "source": [
    "algorithm_config = [{\n",
    "    'name': 'DefaultQuantization',\n",
    "    'params':{\n",
    "        'target_device': 'ANY',\n",
    "        'stat_subset_size':300,\n",
    "        'stat_batch_size':1,\n",
    "        'bits':4,\n",
    "    },\n",
    "}]\n",
    "openvino_model = load_model(model_config=model_config)\n",
    "pipline = create_pipeline(algorithm_config, engine)\n",
    "compressed_model = pipline.run(model=openvino_model)\n",
    "compress_model_weights(compressed_model)\n",
    "compressed_model_paths = save_model(\n",
    "    model=compressed_model,\n",
    "    save_path='models/',\n",
    "    model_name = 'quantization4_swin'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f61349",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model_size = Path('models/quantization4_swin.bin').stat().st_size / (1024 * 1024)\n",
    "quantized_model_size = Path('models/quantization_swin.bin').stat().st_size / (1024 * 1024)\n",
    "\n",
    "print(f'4int IR  size: {original_model_size:.2f} MB')\n",
    "print(f'8int IR size: {quantized_model_size:.2f} MB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "384ed1ff3e149b3f0d14d474c781e2b36d523d38367f2a0fc42966152471552f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
