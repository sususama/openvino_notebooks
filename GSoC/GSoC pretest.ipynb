{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3885b63d",
   "metadata": {},
   "source": [
    "# GSoC pretest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee133ba3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1054aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch.quantization as quantization\n",
    "import matplotlib.pyplot as plt\n",
    "from openvino.runtime import Core\n",
    "from openvino.runtime import serialize\n",
    "from openvino.tools import mo\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from onnxruntime.quantization import QuantType, quantize_dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcc8bcd",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04c9d001",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swin_small_patch4_window7_224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/openvino_env/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "model_names = timm.list_models('swin_small*')\n",
    "for model_name in model_names:\n",
    "    print(model_name)\n",
    "model = timm.create_model('swin_small_patch4_window7_224', pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809d45df",
   "metadata": {},
   "source": [
    "## Run model at ont-quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3069f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 208\n"
     ]
    }
   ],
   "source": [
    "# Load image and preprocess\n",
    "image = Image.open('../notebooks/data/image/coco.jpg')\n",
    "# plt.imshow(image)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.299, 0.224, 0.255])\n",
    "])\n",
    "image = transform(image).unsqueeze(0)\n",
    "\n",
    "# Predict the class of image\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "    pred = output.argmax(dim=1).item()\n",
    "    print(f'Predicted class: {pred}')\n",
    "# After searching, the 208th category in ImageNet is dog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ca845d",
   "metadata": {},
   "source": [
    "## Define convert model to onnx and IR function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5243d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_models(model, model_input, path):\n",
    "    script_model = torch.jit.trace(model, model_input)\n",
    "    torch.onnx.export(script_model,  model_input, path)   \n",
    "    convert_model = mo.convert_model(path)\n",
    "    # Change the .onnx suffix to .xml\n",
    "    IR_path = path[:-4]+'xml'\n",
    "    serialize(convert_model, IR_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b00144",
   "metadata": {},
   "source": [
    "## Convert model and test them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e95ed983",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: 无法创建目录\"models\": 文件已存在\n"
     ]
    }
   ],
   "source": [
    "# Convert model to onnx and IR\n",
    "# If you already have a models directory, place comment the next line of code.\n",
    "!mkdir models\n",
    "model.eval()\n",
    "onnx_path = 'models/swin_small_patch4_window7_224.onnx'\n",
    "model_input = torch.randn(1,3,224,224).cpu() \n",
    "# convert_models(model, model_input,onnx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ec329a",
   "metadata": {},
   "source": [
    "## Use Pytorch built-in quantization to quantize the model and convert them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8576f118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 208\n"
     ]
    }
   ],
   "source": [
    "# ie = Core()\n",
    "# model_ir = ie.read_model(model=(onnx_path[:-4]+'xml'))\n",
    "# model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "model_int8 = quantization.quantize_dynamic(model, dtype=torch.qint8)\n",
    "# quantization.prepare(model, inplace=True)\n",
    "# model_int8 = quantization.convert(model, inplace=True)\n",
    "with torch.no_grad():\n",
    "    output = model_int8(image)\n",
    "    pred = output.argmax(dim=1).item()\n",
    "    print(f'Predicted class: {pred}')\n",
    "# torch.save(model_int8, 'models/quantize_swin.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/layers/layers.0/blocks/blocks.0/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.0/blocks/blocks.0/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.0/blocks/blocks.1/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.0/blocks/blocks.1/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.1/blocks/blocks.0/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.1/blocks/blocks.0/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.1/blocks/blocks.1/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.1/blocks/blocks.1/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.2/blocks/blocks.0/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.2/blocks/blocks.0/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.2/blocks/blocks.1/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.2/blocks/blocks.1/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.2/blocks/blocks.2/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.2/blocks/blocks.2/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.2/blocks/blocks.3/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.2/blocks/blocks.3/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.2/blocks/blocks.4/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.2/blocks/blocks.4/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.2/blocks/blocks.5/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.2/blocks/blocks.5/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.2/blocks/blocks.6/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.2/blocks/blocks.6/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.2/blocks/blocks.7/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.2/blocks/blocks.7/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.2/blocks/blocks.8/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.2/blocks/blocks.8/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.2/blocks/blocks.9/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.2/blocks/blocks.9/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.2/blocks/blocks.10/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.2/blocks/blocks.10/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.2/blocks/blocks.11/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.2/blocks/blocks.11/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.2/blocks/blocks.12/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.2/blocks/blocks.12/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.2/blocks/blocks.13/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.2/blocks/blocks.13/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.2/blocks/blocks.14/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.2/blocks/blocks.14/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.2/blocks/blocks.15/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.2/blocks/blocks.15/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.2/blocks/blocks.16/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.2/blocks/blocks.16/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.2/blocks/blocks.17/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.2/blocks/blocks.17/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.3/blocks/blocks.0/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.3/blocks/blocks.0/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.3/blocks/blocks.1/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers/layers.3/blocks/blocks.1/attn/MatMul_1]\n"
     ]
    },
    {
     "ename": "NotImplemented",
     "evalue": "[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for ConvInteger(10) node with name '/patch_embed/proj/Conv_quant'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplemented\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m quantize_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/quantize_swin.onnx\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m quantize_dynamic(model_input\u001b[38;5;241m=\u001b[39monnx_path,\n\u001b[1;32m      4\u001b[0m     model_output\u001b[38;5;241m=\u001b[39m quantize_path,\n\u001b[1;32m      5\u001b[0m     weight_type\u001b[38;5;241m=\u001b[39mQuantType\u001b[38;5;241m.\u001b[39mQInt8,\n\u001b[1;32m      6\u001b[0m     optimize_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m sess \u001b[38;5;241m=\u001b[39m \u001b[43mort\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInferenceSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquantize_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m input_name\u001b[38;5;241m=\u001b[39msess\u001b[38;5;241m.\u001b[39mget_inputs()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m     11\u001b[0m outpu_name\u001b[38;5;241m=\u001b[39msess\u001b[38;5;241m.\u001b[39mget_outputs()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[0;32m~/miniconda3/envs/openvino_env/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:360\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[0;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m disabled_optimizers \u001b[39m=\u001b[39m kwargs[\u001b[39m\"\u001b[39m\u001b[39mdisabled_optimizers\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mdisabled_optimizers\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwargs \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 360\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_inference_session(providers, provider_options, disabled_optimizers)\n\u001b[1;32m    361\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m    362\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_fallback:\n",
      "File \u001b[0;32m~/miniconda3/envs/openvino_env/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:408\u001b[0m, in \u001b[0;36mInferenceSession._create_inference_session\u001b[0;34m(self, providers, provider_options, disabled_optimizers)\u001b[0m\n\u001b[1;32m    405\u001b[0m     disabled_optimizers \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(disabled_optimizers)\n\u001b[1;32m    407\u001b[0m \u001b[39m# initialize the C++ InferenceSession\u001b[39;00m\n\u001b[0;32m--> 408\u001b[0m sess\u001b[39m.\u001b[39;49minitialize_session(providers, provider_options, disabled_optimizers)\n\u001b[1;32m    410\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sess \u001b[39m=\u001b[39m sess\n\u001b[1;32m    411\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sess_options \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sess\u001b[39m.\u001b[39msession_options\n",
      "\u001b[0;31mNotImplemented\u001b[0m: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for ConvInteger(10) node with name '/patch_embed/proj/Conv_quant'"
     ]
    }
   ],
   "source": [
    "# model_onnx = onnx.load(onnx_path)\n",
    "quantize_path = 'models/quantize_swin.onnx'\n",
    "quantize_dynamic(model_input=onnx_path,\n",
    "    model_output= quantize_path,\n",
    "    weight_type=QuantType.QInt8,\n",
    "    optimize_model=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae025bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(onnx_path)\n",
    "onnx.checker.check_model(onnx_model, full_check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef69a63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_model = mo.convert_model(quantize_path)\n",
    "# Change the .onnx suffix to .xml\n",
    "IR_path = quantize_path[:-4]+'xml'\n",
    "serialize(convert_model, IR_path)\n",
    "# convert_models(torch_model,model_input, quantize_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3a3319",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_int8.eval()\n",
    "quantize_path = 'models/quantize_swin.onnx'\n",
    "# convert_models(model_int8,model_input, quantize_path)\n",
    "# script_model = torch.jit.trace(model_int8, model_input)\n",
    "# torch.onnx.export(script_model,  model_input, quantize_path)\n",
    "# convert_model = mo.convert_model(quantize_path)\n",
    "# # # Change the .onnx suffix to .xml\n",
    "# IR_path = quantize_path[:-4]+'xml'\n",
    "# serialize(convert_model, IR_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9569ae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e953b229",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "384ed1ff3e149b3f0d14d474c781e2b36d523d38367f2a0fc42966152471552f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
